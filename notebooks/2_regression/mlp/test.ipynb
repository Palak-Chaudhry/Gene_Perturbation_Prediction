{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f4057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def build_mlp_pipeline(hidden_layer_sizes=(64, 32),\n",
    "                       alpha=1e-3,\n",
    "                       learning_rate_init=1e-3,\n",
    "                       random_state=42):\n",
    "    \"\"\"\n",
    "    Build a Pipeline: StandardScaler -> MLPRegressor\n",
    "    with the given hyperparameters.\n",
    "    \"\"\"\n",
    "    mlp = MLPRegressor(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=alpha,                  # L2 regularization\n",
    "        learning_rate=\"adaptive\",\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        max_iter=300,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"mlp\", mlp)\n",
    "    ])\n",
    "\n",
    "\n",
    "def cv_tune_mlp_on_train(X_train, y_train,\n",
    "                         param_grid,\n",
    "                         n_splits=5,\n",
    "                         random_state=42):\n",
    "    \"\"\"\n",
    "    Hyperparameter tuning using K-fold CV on the 80% train set.\n",
    "\n",
    "    For each hyperparameter configuration:\n",
    "      - Run K-fold CV on (X_train, y_train)\n",
    "      - Compute mean RMSE across folds\n",
    "    Choose the configuration with the lowest mean RMSE.\n",
    "\n",
    "    Returns:\n",
    "      best_params (dict), best_mean_rmse (float)\n",
    "    \"\"\"\n",
    "    cv = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    best_rmse = np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for params in param_grid:\n",
    "        fold_rmses = []\n",
    "\n",
    "        for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "            X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "            model = build_mlp_pipeline(\n",
    "                hidden_layer_sizes=params[\"hidden_layer_sizes\"],\n",
    "                alpha=params[\"alpha\"],\n",
    "                learning_rate_init=params[\"learning_rate_init\"],\n",
    "                random_state=random_state\n",
    "            )\n",
    "\n",
    "            model.fit(X_tr, y_tr)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "            fold_rmses.append(rmse)\n",
    "\n",
    "        mean_rmse = np.mean(fold_rmses)\n",
    "\n",
    "        if mean_rmse < best_rmse:\n",
    "            best_rmse = mean_rmse\n",
    "            best_params = params\n",
    "\n",
    "    return best_params, best_rmse\n",
    "\n",
    "\n",
    "def run_tf_training_with_heldout_test(X, y, pert_name,\n",
    "                                      train_idx,\n",
    "                                      test_idx,\n",
    "                                      param_grid,\n",
    "                                      cv_splits=5,\n",
    "                                      random_state=42):\n",
    "    \"\"\"\n",
    "    For one TF (one target y):\n",
    "      - Use global train_idx/test_idx to define 80%/20% split.\n",
    "      - On the 80% (train) perform CV-based hyperparameter tuning.\n",
    "      - Retrain best model on full 80%.\n",
    "      - Evaluate once on the 20% held-out test set.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Hyperparameter tuning on train set only\n",
    "    best_params, best_cv_rmse = cv_tune_mlp_on_train(\n",
    "        X_train, y_train,\n",
    "        param_grid=param_grid,\n",
    "        n_splits=cv_splits,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Train final model on full 80% using best hyperparams\n",
    "    final_model = build_mlp_pipeline(\n",
    "        hidden_layer_sizes=best_params[\"hidden_layer_sizes\"],\n",
    "        alpha=best_params[\"alpha\"],\n",
    "        learning_rate_init=best_params[\"learning_rate_init\"],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on the untouched 20%\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    test_corr, _ = pearsonr(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"perturbation\": pert_name,\n",
    "        \"rmse_test\": test_rmse,\n",
    "        \"pearson_corr_test\": test_corr,\n",
    "        \"cv_rmse_mean\": best_cv_rmse,\n",
    "        \"hidden_layer_sizes\": best_params[\"hidden_layer_sizes\"],\n",
    "        \"alpha\": best_params[\"alpha\"],\n",
    "        \"learning_rate_init\": best_params[\"learning_rate_init\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f3c8ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TSS embeddings from: ../../../embeddings/embeddings_enformer_tss.npy\n",
      "Embeddings shape: (19685, 32)\n",
      "Loaded 19685 gene names from metadata\n",
      "X shape: (4773, 32) | Y shape: (4773, 231)\n",
      "Train genes: 3818, Test genes: 955\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================\n",
    "# Load input data\n",
    "# ======================\n",
    "EMBEDDING_TYPE = \"tss\"  # or \"gencode\"\n",
    "\n",
    "input_file = \"../../../data/active_guides_CRISPRa_mean_pop_mean.csv\"\n",
    "mean_pop = pd.read_csv(input_file, index_col=0)\n",
    "\n",
    "# Load embeddings and metadata based on selected type\n",
    "if EMBEDDING_TYPE == \"tss\":\n",
    "    embeddings_path = \"../../../embeddings/embeddings_enformer_tss.npy\"\n",
    "    metadata_path = \"../../../embeddings/enformer_gene_names.txt\"\n",
    "    print(f\"Loading TSS embeddings from: {embeddings_path}\")\n",
    "elif EMBEDDING_TYPE == \"gencode\":\n",
    "    embeddings_path = \"../../../embeddings/embeddings_enformer_gencode.v49.pc_transcripts.npy\"\n",
    "    metadata_path = \"../../../embeddings/gencode.v49.pc_transcripts_gene_names.txt\"\n",
    "    print(f\"Loading Gencode v49 PC transcripts embeddings from: {embeddings_path}\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown embedding type: {EMBEDDING_TYPE}. Use 'tss' or 'gencode'.\")\n",
    "\n",
    "embeddings = np.load(embeddings_path, allow_pickle=True)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "meta_table = pd.read_csv(metadata_path, sep=\"\\t\", header=None, names=[\"index\", \"gene\"])\n",
    "print(f\"Loaded {len(meta_table)} gene names from metadata\")\n",
    "\n",
    "embeddings = pd.DataFrame(embeddings, index=meta_table['gene'])\n",
    "\n",
    "# Align datasets: same genes in embeddings and mean_pop\n",
    "embeddings = embeddings.groupby(embeddings.index).mean()\n",
    "embeddings.head()\n",
    "common_genes = mean_pop.columns.intersection(embeddings.index)\n",
    "X = embeddings.loc[common_genes]            # genes x features\n",
    "Y = mean_pop[common_genes].T               # genes x perturbations\n",
    "\n",
    "perturb_names = Y.columns\n",
    "\n",
    "print(\"X shape:\", X.shape, \"| Y shape:\", Y.shape)\n",
    "\n",
    "# ======================\n",
    "# Create a single 80/20 train/test split for genes\n",
    "# ======================\n",
    "n_genes = X.shape[0]\n",
    "all_indices = np.arange(n_genes)\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    all_indices,\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train genes: {len(train_idx)}, Test genes: {len(test_idx)}\")\n",
    "\n",
    "# Convert X to numpy once\n",
    "X_np = X.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b38011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loveslide_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
